{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac108ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link: https://repositorio.lamolina.edu.pe/handle/20.500.12996/151\n",
      "62 tesis en\n",
      "4 paginas\n",
      "https://repositorio.lamolina.edu.pe/handle/20.500.12996/151\n",
      "https://repositorio.lamolina.edu.pe/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "import math as mt\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "\n",
    "\n",
    "link  = input(\"link: \")\n",
    "url=link\n",
    "linkss = re.sub(\".pe(.*)\",\".pe/\",link)\n",
    "link=url+\"/recent-submissions?offset=0\"\n",
    "html = urllib.request.urlopen(link, context=ctx).read()\n",
    "tesis= re.findall(b'<p.class.*de.(.*)<.p', html)\n",
    "for z in tesis:\n",
    "    i=z.decode()\n",
    "    print(i,\"tesis en\")\n",
    "#numero = 1\n",
    "#print(mt.ceil(numero))\n",
    "paginas=62/20\n",
    "paginas=mt.ceil(paginas)\n",
    "\n",
    "print(paginas,\"paginas\")\n",
    "#link de tesis= https://repositorio.lamolina.edu.pe/handle/20.500.12996/151\n",
    "print(url)\n",
    "print(linkss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6889a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2022 1\n",
      "2\n",
      "Identificación de clientes en campañas para una entidad financiera usando el método Stacking 2\n",
      "3\n",
      "Alvarez Chancasanampa, Julio César 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Miranda Villagómez, Clodomiro Fernando 5\n",
      "6\n",
      "La presente investigación, tiene como objetivo general determinar si el método de ensamble Stacking predice con mayor precisión a los clientes potenciales a quienes se les otorgará o desembolsará préstamos en las ofertas de campaña de una entidad financiera, que los algoritmos de aprendizaje supervisado de Machine Learning: Random Forest, Regresión Logística y Árbol de Decisión. La evaluación se realizó comparando el método Stacking con los modelos individuales de Regresión Logística, Árbol de Decisión y Random Forest. Para dicha evaluación se usaron los indicadores Auc, Gini, Logloss y Kolmogorov. Los resultados de sensibilidad en orden de importancia que se obtuvieron con los modelos estadísticos fueron lo siguiente: Regresión Logística 88.9%, seguido del método Stacking con 87.9%, luego el Árbol de Decisión con un 84% y por último Random Forest con un 82.7%. Mientras, que al evaluar la especificidad el de mayor importancia fue el modelo de Random Forest con un 84.8%, Árbol de Decisión 82.8%, método Stacking 81.6% y por último Regresión Logística 78.4%. Respecto a los indicadores evaluados, el que presentó mayor Auc es el método Stacking 0.9117, seguido de Random Forest con un 0.9074, la Regresión Logística reportó un 0.9064 y Árbol de Decisión 0.9074. Con respecto al indicador Gini, el que tiene mayor Gini es el método de Stacking con 0.8235, seguido de Random Forest con un 0.8148, la Regresión Logística cuyo resultado fue de 0.8128 y en última posición el Árbol de Decisión con 0.7885. Con relación al indicador Logloss, el que mostró mejor desempeño fue el método Stacking con 0.3177, seguido de Random Forest con 0.3435, Árbol de Decisión 0.3886 y Regresión Logística 0.3959. Finalmente, con respecto al indicador Kolmogorov, el que tiene mejor resultado es el método Stacking con 0.7124, seguido por Random Forest con 0.7028, Árbol de Decisión 0.6907 y por último la Regresión Logística con 0.6751. 6\n",
      "1\n",
      "2022 1\n",
      "2\n",
      "Implementación de una solución de business intelligence para la toma de decisiones en el servicio de consulta externa de un hospital 2\n",
      "3\n",
      "Tinco Curi, Elizabeth Irene 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "Actualmente el sector salud dispone de una gran cantidad de información, la cantidad masiva de datos que se genera es muy compleja, los hospitales y otras organizaciones del sector necesitan un entorno que apoye las prácticas diarias de los médicos, la administración y el resto del personal de atención médica. La información utilizada para la generación de reportes proviene de sistemas transaccionales (Sistema de gestión hospitalaria), el cual almacena información referente al paciente y a su atención. El proceso de elaboración del reporte se realiza de manera manual-operativa ocasionando demoras e inconsistencias de información. El presente trabajo tiene como finalidad la implementación de una solución de Business Intelligence con el objetivo de obtener información única, limpia confiable y actualizada para facilitar la toma de decisiones en el servicio de consulta externa de un hospital, para ello se utilizó la metodología de Ralph Kimball que es la que se ajusta a las necesidades de la empresa, esencialmente por su esquematización a nivel de áreas específicas, tiempo de desarrollo y complejidad. Para el proceso de análisis y depuración de datos, así como para extracción, transformación y carga de procesos de actualización del Datamart para el servicio de consulta externa se hizo uso de la herramienta SQL Server Enterprise 2016. Los resultados de la implementación de la solución de Business Intelligence permitieron generar reportes y carga de datos en menor tiempo, lo que conlleva a obtener información confiable y concisa. El usuario podrá hacer uso de la información mediante reportes diseñados de acuerdo con sus necesidades. Finalmente se evidenció que el uso de herramientas de Business Intelligence tuvo impactos positivos en el área involucrada, facilitando la organización de la información a través de las herramientas de visualización y análisis de datos como: Excel, Reporting Services y el Power BI lo que permitió tomar decisiones acertadas en beneficio de los usuarios. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Identificación de clientes que realizaron fuga de equipos móviles en una empresa de telecomunicaciones utilizando el algoritmo Random Forest 2\n",
      "3\n",
      "Marquez Meza, Francisco 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "El presente trabajo tiene como objetivo ilustrar el proceso de construcción del modelo predictivo que  permitió  detectar  los  casos  fuga  de equipos móviles en la Empresa, de forma anticipada al monitoreo existente, y en concreto para  el  canal  de  venta  Canal  1  y  para  la  ventana  de  monitoreo  de  45  días.  Lo anterior, mediante la utilización del algoritmo de aprendizaje  automático  denominado  Random Forest. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Estimación del monto de siniestros ocurridos y no reportados para el SOAT con el método Double Cchain Ladder 2\n",
      "3\n",
      "Alarcón Pimentel, Sandra Elena 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "El sector asegurador peruano es supervisado en todo momento por la Superintendencia de Banca y Seguros – SBS. Esta entidad vigila y evalúa el comportamiento, prácticas e información relacionada a las aseguradoras con el objetivo de fomentar la rentabilidad, transparencia y una mayor protección de asegurados y beneficiarios. Para lograrlo las aseguradoras deben manejar métodos de estimación de provisiones (reservas) técnicas que contengan fundamentos estadísticos y que los resultados obtenidos sean lo más certero posible. En esta monografía, se presenta la metodología y la aplicación del método estocástico Double Chain Ladder - DCL para el Seguro Obligatorio de Accidentes de Tránsito – SOAT, siendo este tipo de método el más ventajoso para la estimación de pagos futuros de siniestros. Los resultados con la metodología DCL para estimación de la provisión de pagos futuros de los accidentes en el SOAT para la empresa aseguradora, se obtuvieron valores estimados para las reservas de siniestros ocurridos (RBNS) de S/. 293,206 y para las reservas de siniestros ocurridos y no reportados (IBNYR) de S/. 48,826 y para el total de las  reservas IBNR en S/. 342,033. En comparación, con el método Double Chain Ladder se obtuvo una reserva de S/. 342,033 menor que con el método Chain Ladder S/349,117. En la repartición de los siniestros, el 90% es de las reservas son RBNS y un 10% de las IBNYR, lo que implica que la gran mayoría de siniestros ya han sido notificados a la compañía, pero hay un retraso en la liquidación 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Registro de los planes operativos y estratégicos usando el aplicativo CEPLAN V 1.0 2\n",
      "3\n",
      "Flores Santos, José Alberto 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "El Centro Nacional de Planeamiento Estratégico (CEPLAN) es un organismo técnico especializado que ejerce la rectoría efectiva del Sistema Nacional de Planeamiento Estratégico conduciéndolo de manera participativa, transparente y concertada, contribuyendo así al mejoramiento de la calidad de vida de la población y al desarrollo sostenible del país. Ejercer la rectoría y orientar a las entidades del Sistema Nacional de Planeamiento Estratégico (SINAPLAN), en una gestión eficaz y eficiente, para alcanzar la visión concertada del futuro del país permitiendo el desarrollo armónico y sostenible. Dentro de sus funciones se encuentra desarrollar metodologías e instrumentos técnicos para asegurar la consistencia y coherencia del plan estratégico de desarrollo nacional. Asesorar a las entidades del estado y los gobiernos regionales y orientar a los gobiernos locales en la formulación, el seguimiento y la evaluación de políticas y planes estratégicos de desarrollo para lograr los objetivos estratégicos de desarrollo nacional propuestos oportunamente. Efectuar el seguimiento y la evaluación de la gestión estratégica del estado en forma continua. Dentro de sus órganos institucionales se encuentra la oficina de Administración entre cuyas principales funciones está la automatización y la mejora continua de procesos informáticos que sirva de apoyo al público en general. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Diseño e implementación de un Datamart para el área de análisis de una instancia técnica del sector educativo 2\n",
      "3\n",
      "Ayala Flores, Karen Angela 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "La Instancia Técnica del sector educativo al que hace referencia el presente proyecto, posee gran cantidad de datos históricos producto de las evaluaciones de logros de aprendizaje que realiza anualmente. Sin embargo, no cuenta con un almacén de bases de datos estructurado, integrado y ordenado que permita el uso de esta información de manera óptima. En consecuencia, demanda de mucho tiempo atender los pedidos de información y generar reportes, además de que por ser este un proceso manual se incurre muchas veces al error. Por todo ello, surgió la necesidad de incorporar soluciones tecnológicas e innovadoras, como es el proceso de Inteligencia de Negocios. Por consiguiente, el presente proyecto tiene como objetivo principal la implementación de un Datamart para el área de Análisis de la Instancia Técnica, con la finalidad de gestionar la información y apoyar en la toma de decisiones. Esta implementación se realizó empleando la metodología de Ralph Kimball describiendo detalladamente cada etapa. Además, cabe mencionar, que para la implementación del Datamart se utilizaron únicamente softwares libres, como el Pentaho Data Integration – Kettle (herramienta de extracción, transformación y carga) y el MySQL Workbench (herramienta de diseño y gestión de bases de datos). Adicionalmente, para elaborar los reportes e informes, se utilizaron las herramientas Microsoft Excel y Power BI. La implementación del Datamart permitió automatizar procesos, con la creación de consultas para la atención de los pedidos de información y generación de reportes, originando que los tiempos de respuesta disminuyeran. Además, toda la información ingresada al Datamart fue validada por lo que ahora es confiable y consistente. Se generaron también accesos para los usuarios en base a sus perfiles. Adicionalmente, se crearon Dashboards para el monitoreo, análisis y visualización de los principales indicadores y métricas para la toma de decisiones acertadas. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Predicción del riesgo de incumplimiento en el pago de los créditos del portafolio de una entidad financiera utilizando regresión logística 2\n",
      "3\n",
      "Miranda Pilco, Adriana 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "El éxito de toda entidad financiera radica en la adecuada gestión de los riesgos a los que se encuentra expuesta, siendo uno de ellos el Riesgo de Crédito definido como la posibilidad de pérdida a consecuencia del incumplimiento de las obligaciones por parte del prestatario. Las herramientas analíticas usadas en la gestión de este tipo de riesgos han ido evolucionando a lo largo del tiempo e incluyendo a la estadística y la minería de datos como parte de estas. En esta memoria de Trabajo de Suficiencia Profesional se describe como la aplicación de la metodología de Credit Scoring conjuntamente con la metodología de minería de datos CRISP DM para la construcción de un modelo de riesgo comportamental en una entidad financiera, permitió obtener un indicador de gini de 64% y segmentar de mejor manera al portafolio de clientes de dicha entidad al incrementar en un 20% la participación de mejores clientes. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Propuesta de estrategia de monitoreo transaccional anti lavado de activos empleando el método de Ward y el teorema de Chebyshev 2\n",
      "3\n",
      "Romero Domínguez, Adrián 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "La adecuada identificación de casos de investigación y la detección de operaciones sospechosas son pilares del sistema de monitoreo anti lavado de activos. En esa línea, el presente estudio ha propuesto implementar dentro del sistema de monitoreo anti lavado una estrategia basada en una segmentación empleando el método de Ward, definiendo umbrales de alertamiento mediante el teorema de Chebyshev. Mediante el método de Ward se logró segmentar el portafolio de clientes de depósitos de plazo fijo en cuatro segmentos homogéneos al interno pero heterogéneos entre sí. Luego en cada uno de estos se definieron sendos umbrales de alertamiento partiendo del teorema de Chebyshev. Esto permitiría a la empresa donde se realizó el estudio acentuar y priorizar los casos a investigar a fin de identificar con mayor celeridad el riesgo de lavado de activos y reportar los casos a la autoridad competente, objetivo central de la unidad de Prevención de Lavado de Activos. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Diseño e implementación de un sistema de información web en una empresa de medios de comunicación de Perú 2\n",
      "3\n",
      "Jiménez Saravia, Juan Gabriel 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "El presente trabajo consiste en diseñar e implementar un sistema de información Web para una empresa líder en el mercado de investigación de medios de comunicación en América Latina, este sistema tiene como objetivo la automatización y digitalización de los procesos manuales que se ejecutan en el área de operaciones; entre estos procesos tenemos: control de calidad, sistemas para entrada y análisis de datos, reportes automatizados, etc. Para el desarrollo del sistema utilicé el gestor de base de datos MySQL, los lenguajes de programación PHP y JavaScript; y para el desarrollo de reportes y tableros de control automáticos utilicé el Software Power BI. Con el sistema implementado logre reducir el uso de papel a 0, eliminar el proceso de digitación de los formatos de recolección de datos, reducir los tiempos de los procesos de campo, además de consolidar y estandarizar todos los datos de la operación en una única base de datos MySQL. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Aplicación del análisis Pathway para incrementar la predisposición de compra de productos de belleza 2\n",
      "3\n",
      "Huaman Torres, Karen Lizbeth 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Gamboa Unsihuay, Jesús Eduardo 5\n",
      "6\n",
      "Las diferentes empresas tienen como uno de sus principales retos motivar la compra de los distintos bienes o servicios que brindan. De allí que surge la importancia de entender lo que el público valora al momento de elegir un producto. Teniendo ello en cuenta, el presente trabajo tiene como objetivo describir el camino crítico respecto a los factores que se deben accionar para lograr incrementar la predisposición de compra de productos de una marca de belleza. Su análisis se desarrolló en 3 fases: la primera consistió en un Análisis Factorial Exploratorio (AFE), donde los factores identificados permitieron reducir el número de variables que se consideraron para el análisis y cumplieron así el rol de variables latentes en el modelo. En la segunda fase se desarrolló un Análisis Factorial Confirmatorio (AFC), con el fin de confirmar el modelo obtenido en la fase previa, donde se verificó que la relación entre las variables y su respectivo factor era significativa. Por último, en la fase 3 se identificó a través del análisis Pathway, la relación de las variables que se consideraron en el modelo y cuál fue su efecto sobre la variable objetivo (predisposición de compra). Cabe mencionar, que para entender la variable objetivo se consideró como variables observables la significancia, diferenciación y presencia de una marca, mientras que como variables latentes o factores: el aspecto emocional, oferta, eficacia y vanguardia, que fueron medidas desde la percepción del consumidor. Es así como se identificó el efecto para todas las variables incluidas en el modelo sobre la predisposición de compra, y se concluyó que el efecto de mayor magnitud corresponde al factor emocional, lo que resultaría clave para incentivar la compra de los productos de alguna marca de belleza. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Análisis y pronóstico de la recaudación del impuesto vehicular municipal mediante la metodología Box - Jenkins 2\n",
      "3\n",
      "Huamán Inga, Cinthia Katheryn 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "El principal objetivo del presente trabajo de suficiencia profesional es: determinar el mejor modelo que se ajuste a las características de la recaudación del Impuesto Vehicular de la Provincia de Lima mediante la técnica estadística de series de tiempo, con la finalidad de pronosticar el registro de ingresos de dicho concepto en tiempos futuros. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Modelo de factores asociado al desarrollo infantil temprano de niños peruano con indicadores  del Instituto Nacional de Estadística e Informática 2\n",
      "3\n",
      "Guzmán Alanya, Miguel Alonso 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "En el puesto de analista de procesamiento de información en el Ministerio de Desarrollo e Inclusión Social se realizaron las siguientes funciones: apoyar en el cálculo de indicadores, generar reportes con las bases de datos de los sectores relacionados con salud, educación y agua y asistir técnicamente al equipo del Fondo de Estímulo al Desempeño y Logro de Resultados Sociales. En los últimos cinco años, se desarrollaron diversas tareas que pudieron ser realizadas adecuadamente con los contenidos de los siguientes cursos: Estadística Aplicada, Técnicas de Muestreo, Bases de datos y Técnicas Multivariadas. En el Ministerio de Desarrollo Inclusión Social, en el área del Fondo de Estímulo al Desempeño y Logro de Resultados Sociales, hasta el año 2019, no se contaba con una metodología capaz de reducir el gran número de indicadores relacionados con el Desarrollo Infantil Temprano, con la finalidad de ser usados posteriormente para el cálculo de los incentivos monetarios a los gobiernos regionales. En esta área se contaba con el apoyo de especialistas temáticos en salud, agua y educación, que explicaban la finalidad y utilidad de cada indicador, pero solo contaban con su criterio y experiencia para seleccionar los indicadores. Por lo tanto, evaluada la necesidad del área, se decidió a realizar un análisis factorial exploratorio, utilizando indicadores de agua, educación y salud publicados por el Instituto Nacional de Estadística e Informática relacionado con el Desarrollo Infantil Temprano, el cual permitió obtener un número reducido de indicadores con sustento técnico. El trabajo monográfico se encuentra dividido en seis partes, una vez presentado la introducción se pasará al marco teórico, seguido del marco metodológico, posteriormente, los resultados y discusión, luego las conclusiones y recomendaciones y finalmente las referencias bibliográficas. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Identificación de perfiles de los Centros de Educación Técnico - Productiva Públicos usando indicadores de condiciones básicas de calidad mediante clúster bietápico 2\n",
      "3\n",
      "Tonconi Calisaya, Cesar Anthony 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Valencia Chacón, Raphael Félix 5\n",
      "6\n",
      "El aumento continuo de grandes volúmenes de datos y la importancia de la utilización de éstos, junto con la búsqueda de información se han vuelto un gran reto que las grandes entidades públicas hoy en día quieren superar. En la actualidad las entidades públicas conocen la importancia que tiene el almacenamiento, la captura de datos y el beneficio que le puede resultar si se explotan correctamente. Este estudio presenta una técnica estadística para segmentar e identificar perfiles de los Centros de Educación Técnico-Productiva (CETPRO) públicos. El conjunto de datos está conformado por información de todos los CETPRO públicos a nivel nacional recogida en el 2019. El conjunto de datos inicial estaba compuesto por 704 instituciones educativas; posteriormente, luego de proceder con el análisis exploratorio y la limpieza de datos, correspondiente a la eliminación de datos outliers y faltantes; se trabajó un conjunto de datos compuesto por 684 instituciones. Se aplicó análisis de clúster bietápico, que es una técnica de segmentación que permite trabajar con variables cuantitativas y categóricas. El resultado de la aplicación de la técnica brindó 2 conglomerados: el primero con 324 CETPRO (47.4%), el segundo conglomerado con 360 (52.6%). Después de esto se procedió a describir los perfiles de cada conglomerado y se identificaron las principales características en función de las variables relacionadas a las 5 condiciones básicas de calidad planteadas. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Segmentación de usuarios que visitan el sitio web de una empresa utilizando la regresión logística con la técnica de sobremuestreo 2\n",
      "3\n",
      "Tasayco Silva, Carlos Marcial 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "El trabajo de monografía se desarrolló en una empresa consultora líder en Latinoamérica especializada en soluciones analíticas para las áreas de marketing digital de empresas multinacionales. El trabajo consistió en la implementación y automatización de un modelo de regresión logística binaria para determinar los segmentos de los usuarios que visitan la Web de la empresa. Para realizar el modelo estadístico mencionado se empezó desde el análisis univariante de cada variable independiente, seguido por una técnica de sobremuestreo “SMOTE” para evitar el desbalance de las clases en la variable dependiente, se realizó además una matriz de confusión en la cual se obtuvo una precisión del 76%, hasta finalmente validar la predictibilidad analizando la curva ROC. Los resultados de la investigación ayudaron a demostrar y concluir que existen variables que son significativas para determinar si un usuario que visita la Web realiza una transacción. Por ejemplo: Los usuarios que usan canales orgánicos sin medios publicitarios como referencia y directo o pagados como las redes sociales contribuyen negativamente a la probabilidad de que el usuario haga la transacción, así como también se observó que tanto el tiempo de la visita o si el usuario visita la Web recurrentemente contribuyen a que la probabilidad de hacer la transacción se incremente. Finalmente, la segmentación que se realizó basado en las puntuaciones calculadas por la regresión logística binaria para tener tres segmentos bien diferenciadas que son alto, medio y bajo probabilidad. Al final del trabajo, la empresa aceptó y mostró su satisfacción con los resultados obtenidos. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Diseño muestral para determinar la demanda nacional de hoja de coca destinada al uso lícito 2\n",
      "3\n",
      "Arroyo Maury, Paola Cinthya 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "El Instituto Nacional de Estadística e Informática en coordinación con la Comisión Nacional para el Desarrollo y Vida sin Drogas, ejecutó en el año 2019 por tercera vez en el Perú la encuesta nacional sobre consumo tradicional de hoja de coca, cuyo objetivo fue conocer la cantidad de hoja de coca adquirida en su forma natural a fin de caracterizar a la población para el análisis y diseño de políticas públicas. Para aplicar esta encuesta por muestreo, se llevó a cabo un plan de muestreo o diseño muestral complejo. La metodología utilizada para el diseño muestral se basó en cinco pasos. El primer paso fue para determinar la población objetivo, la cual estuvo constituida por las personas de 12 años y más de edad residentes habituales en las viviendas particulares del área urbana y rural del país. En el segundo paso se determinó el marco muestral, el cual se basó en un marco de áreas (conglomerados) y de lista (viviendas). El tercer paso fue elegir la técnica de muestreo, el cual consistió en un muestreo bietápico. El cuarto paso consistió en determinar el tamaño óptimo de muestra. La muestra estimada para la encuesta fue de 8600 viviendas, distribuidas en 849 conglomerados. El quinto paso consistió en llevar a cabo el proceso de muestreo. Los dominios de estudio fueron: nacional, nacional urbano, nacional rural, costa, sierra, selva y el área metropolitana de Lima y Callao. Las entrevistas fueron realizadas en los meses de julio y agosto del 2019, lográndose entrevistar a 8 371 viviendas, y un total de 23041 personas con una tasa de no respuesta de 2.7% y un error de marco del 12%. La estimación de la población objetivo que adquiere hoja de coca fue de 3 millones 692 mil 694 personas y la cantidad adquirida corresponde a 9558,62 TM. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Predicción de la prima de los clientes de una compañía aseguradora usando el modelo lineal generalizado Tweedie 2\n",
      "3\n",
      "Armas Alvarado, Alberto Reimundo 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "El objetivo de este trabajo es predecir la prima de los clientes de seguros vehiculares usando el modelo lineal generalizado Tweedie. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Validación para la determinación de minerales: calcio, hierro, magnesio, manganeso y zinc, en galletas con el método AOAC 985.35 2\n",
      "3\n",
      "Palomino Salazar, Leslie Liliana 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "El objetivo general de la memoria de trabajo de suficiencia profesional es validar la robustez del método para la determinación de minerales: Calcio, Hierro, Magnesio, Manganeso y Zinc, en galletas con el método AOAC 985.35 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Evaluación del impacto de la fortificación de la harina de trigo en el nivel de anemia utilizando la técnica PSM 2\n",
      "3\n",
      "Rivera Huamaní, Gianinna Magaly 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "La presente monografía de trabajo de suficiencia profesional se realizó en base a una investigación, que permite evaluar el impacto de la intervención de salud pública de fortificación de la harina de trigo con hierro sobre la prevalencia de anemia y deficiencia de hierro; en mujeres en edad fértil no gestantes entre los 15 a 49 años de edad de Chiclayo e Ica. Donde se evidencia a la anemia como un problema de salud pública que afecta al 21% de dicho segmento de la población de nuestro país. El diseño usado en la investigación fue cuasiexperimental, analítico y transversal; se realizó el análisis secundario de la base de datos recopilados en la encuesta de evaluación de impacto de la fortificación de la harina de trigo diseñada por el Centro Nacional de Alimentación y Nutrición del Instituto Nacional de Salud. En el marco metodológico, se empleó la técnica Propensity Score Matching para evaluar el efecto de la intervención para las participantes (consumo alto de alimentos derivados de la harina de trigo fortificada con hierro), mediante la utilización de covariables de un grupo de controles que no participan en la intervención, asignando a cada participante un par o medida de “control” lo más parecido a ella en términos de sus características, mediante el puntaje de propensión o probabilidad de participación en el programa. Los resultados muestran que la fortificación de la harina de trigo con hierro no alcanzó un impacto significativo sobre la prevalencia de anemia y déficit de hierro en el grupo de estudio. Dado que la fortificación no dio los resultados esperados en la mejora de la prevalencia de anemia, sería recomendable que se realicen inspecciones de control de calidad de la harina de trigo en los productos terminados de consumo masivo. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Modelo de ecuaciones estructurales en innovación de empresas de manufactura peruanas con información del Instituto Nacional de Estadística e Informática 2\n",
      "3\n",
      "Soria Gomez, Eduardo Javier 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "La presente monografía se elaboró en base a una investigación cuyo objetivo era verificar el modelo de ecuaciones estructurales de la innovación tecnológica en empresas de manufactura peruana utilizando los resultados de la encuesta Nacional de Innovación en la Industria Manufacturera aplicada en el 2012 por el Instituto Nacional de Estadística e Informática. La evaluación de los resultados se realizó mediante con el software SmartPLS, para la investigación del paper Analyzing technological innovation in low and medium-low tech Peruvian manufacturing companies, desarrollado por un grupo de investigadores Christian M. Ringle, Sven Wende y Jan-Michael Becker de universidades europeas. Dicho estudio se enmarca en la política de promoción de investigación de la Facultad de Ingeniería de la Universidad ESAN. Como parte de la promoción de la investigación se identificó la necesidad de proponer un modelo de ecuaciones estructurales que explique la innovación tecnológica de empresas de manufactura peruanas, mediante las interrelaciones de las variables latentes capacidad de absorción, fuentes de información y adquisición tecnológica. La investigación se realizó con 856 empresas. Los resultados más importantes de la investigación muestran que las interrelaciones del modelo propuesto de ecuaciones estructurales verifican que la innovación tecnológica de empresas de manufactura peruanas se explica únicamente por las variables latentes capacidad de absorción y adquisición tecnológica, y que dicho modelo presenta un buen ajuste, debido a que el indicador coeficiente de determinación ajustado fue 0.520; y el indicador GoF fue de 0.5, por ser mayor al valor óptimo de 0.31. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Imputación de datos faltantes en los ingresos por hogar en la Enaho utilizando el método del K-vecino más cercano 2\n",
      "3\n",
      "Collazos Tuesta, Oscar Ronald 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "La Encuesta Nacional de Hogares (ENAHO), es el instrumento que utiliza el Instituto Nacional de Estadística e Informática (INEI) para recopilar a nivel nacional los datos de los hogares sobre su condiciones económicas, educativas, salud, etc. y que permiten generar indicadores que miden el estado y la evolución de la pobreza, el bienestar y las condiciones de vida de los hogares del Perú, así como para efectuar diagnósticos y medir el alcance de los programas sociales (alimentarios y no alimentarios) en la mejora de las condiciones de vida de la población peruana. Sin embargo, un problema que debe enfrentar la ENAHO es la no respuesta total o parcial en las unidades de muestreo (no respuesta en unidades) o en una pregunta específica (no respuesta por ítem); sobre todo a las preguntas referidas a los ingresos de los hogares. Para el tratamiento de los datos faltantes, se han propuesto una variedad de métodos que comprenden desde el más simple que consiste en la eliminación de las observaciones que tengan algún dato faltante en una de las variables hasta métodos más consistentes basados en un proceso de imputación con los datos faltantes a partir de los datos completos. El objetivo de esta investigación es presentar y aplicar los métodos de imputación de la media y mediana, el método Hot-Deck y el k vecino más cercano para estimar los datos faltantes del Ingreso por hogar en la ENAHO 2017 trimestre 3. Los resultados indican que los datos faltantes del ingreso tienen un mecanismo MCAR. La estimación del intervalo de confianza del 95% para la media de los ingresos imputados, tuvieron amplitudes por el método de la media 131,41 (el menor) mientras que por el k vecino más cercano fue 139,4. Para estimación de la desviación estándar del ingreso, fue el menor para la media 92,97 y k vecino más cercano 100,99. Los resultados de la comparación de los métodos de imputación, fueron usando los datos completos para generar una muestra aleatoria de datos faltantes artificiales y luego se hallaron el Cuadrado Medio del Error (ECM) y correlaciones con los datos observados e imputados para cada método. El método del k vecino más cercano tuvo los menores valores de ECM 1412,6 y 444,4 para la media y mediana; mientras que los otros métodos sus valores fueron por la media 1504,5; por la mediana 1619,9 y por el Hot-Deck 1963,7. Los coeficientes de correlaciones resultaron con valores muy similares, para k vecino más cercano 0,968 con la media y 0,964 con la mediana. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Diseño y análisis de un sistema de información para el registro nacional de personas certificadas del SINEACE 2\n",
      "3\n",
      "Castro Rojas, Jheynner Adler 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "Hoy en día, la información de las organizaciones es uno de los activos más importantes para estas, y las entidades del Estado no son ajenas a ello. Sin embargo, en ocasiones estas tienen serios problemas para salvaguardar su propia información. El Sistema Nacional de Evaluación, Acreditación y Certificación (SINEACE), ante la falta de un sistema diseñado a medida de sus necesidades, hacía uso de ficheros en Excel para almacenar información referente al proceso de certificación de competencias. El presente trabajo busca diseñar y analizar cada tramo del desarrollo de los procesos de certificación, haciendo uso de una adaptación del ciclo de vida del desarrollo de sistemas de información, con la finalidad establecer con claridad el adecuado procedimiento de flujos de datos y optimizar los diversos procesos para la recuperación de estos, involucrando a todas las partes interesadas; y logrando como resultado el diseño de un sistema adecuado a las necesidades de la institución, incluyendo el modelo entidad-relación, que permita aclarar las relaciones entre las diversas entidades involucradas en el proceso de captura de información; así como, también, un prototipo que ayude al posterior desarrollo del sistema que tenga la función de gestionar la información de los procesos de certificación de competencias 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Estimación del capital económico por riesgo operacional financiero usando el método de distribución de pérdidas con convolución Poisson y Log-normal 2\n",
      "3\n",
      "Polo Sánchez, Silvia Patricia 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "En una institución bancaria el área de Administración de Riesgo Operacional es la encargada de gestionar, evaluar, medir y supervisar los riesgos operacionales en todas las áreas de la financiera en base al cumplimiento de la normativa regulatoria, en el Perú el ente regulador bancario es la SBS (comité de superintendencia de banca y seguros), el cual hace el requerimiento del capital económico por riesgo operacional a las instituciones bancarias, este capital permite a la empresa provisionar un dinero adecuado para cubrir las existencias de pérdidas generadas por riesgos operacionales (fallas en procesos, errores humanos, fallas en sistemas, etc). Se aplicó para estimar el capital económico por riesgo operacional el método de medición avanzada (AMA) usando el modelo de distribución de pérdidas (LDA) con convolución Poisson y Log-Normal. Los resultados obtuvieron una estimación de la pérdida esperada de 154,000,000 soles, pérdida no esperada de 27,000,000 soles y un capital económico de 181,000,000 soles; obteniendo la empresa bancaria un ahorro de capital de 54,000,000 soles. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Segmentación de lectores digitales registrados de un sitio web informativo con el algoritmo de análisis Cluster k-means 2\n",
      "3\n",
      "Clemente Rivera, Brian Erick 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "Debido a la gran diversidad de negocios de la empresa, un importante grupo de medios de comunicación, es que se genera mucha información, cada vez más específica y detallada; y es aquí en donde entra la gerencia de Inteligencia de Negocios, la cual centraliza todos estos datos provenientes de distintas fuentes y plataformas con la finalidad de analizarlos y brindar soporte a las diferentes áreas que requieran de algún análisis a detalle para sustentar una venta, una adquisición, el desarrollo de proyectos, etc. Una de estos requerimientos especializados tiene que ver con la integración de datos de distintos orígenes tanto digitales como los que se generan en los sitios web, las redes sociales; o las tradicionales como los ingresos que genera la publicidad para la compañía, base de datos de audiencia, entre otros, ya que permitirán abordar análisis más complejos para encontrar hallazgos más específicos, diferenciales y relevantes. La presente monografía aborda el desarrollo de una nueva metodología de segmentación de usuarios registrados en la página web, en la cual se ha planteado considerar el tipo de contenido que ellos visitan según la sección en la que están alojadas las notas y complementándolos con la información personal, sociodemográfica, de ubicación y otras disponibles en el negocio, todo esto apoyado en el análisis clúster, específicamente el algoritmo k-means. Para el preprocesamiento de datos, limpieza, construcción del conjunto de datos y ejecución de la metodología se utilizó el software R, que posee múltiples funciones que ayudaron con estas tareas. Estas seis agrupaciones encontradas permitirán ofrecer a los clientes un nuevo producto comercial, que además otorgará una ventaja para los clientes ya que podrán especificar la audiencia específica a la que quieren impactar mejorando significativamente los resultados que se obtendrían a diferencia del método tradicional de publicidad digital. 6\n",
      "1\n",
      "2021 1\n",
      "2\n",
      "Clasificación de los adolescentes infractores del centro juvenil de diagnóstico y rehabilitación de lima utilizando partición alrededor de medoides (PAM) 2\n",
      "3\n",
      "Che Piu Deza, Gilda 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Gamboa Unsihuay, Jesús Eduardo 5\n",
      "6\n",
      "El país sufre de delincuencia desde hace años, percepción que se mantiene alrededor del 40% de los mayores de edad, incluso a marzo del año 2020. Algunos actos delictivos son cometidos por adolescentes, quienes son vulnerables física y psicológicamente a cometer infracciones, afectando a toda la sociedad, sin embargo, información sobre esta realidad nacional es limitada, por ello el este estudio tiene el objetivo de segmentar a los adolescentes infractores del Centro Juvenil de Diagnóstico y Rehabilitación de Lima utilizando el algoritmo PAM (Partición Alrededor de Medoides), que conglomera los datos en k grupos, donde k es calculado aplicando el método de la silueta. Se analizaron variables relacionadas al listado de factores de riesgo del Plan Nacional de Prevención y Tratamiento del Adolescente en Conflicto con la Ley Penal, y de 790 internos, se obtuvieron 2 segmentos, el primero compuesto por adolescentes internos por diversas infracciones (contra el patrimonio, contra la vida el cuerpo y la salud, contra la libertad y contra la seguridad pública) y motivos (lucro personal, emoción violenta, ajuste de cuentas y venganza), los que nunca integraron una familia nuclear y lo que prácticamente pasaron toda su adolescencia con sus padres, con historial de consumo de drogas y/o alcohol, pero que nunca se escaparon de su casa, ni tenían familia con historial penitenciario, ni mejores amigos infractores y vivían en zonas tranquilas; en cambio el segundo segmentó a los que en su mayoría cometieron infracciones contra el patrimonio, que vivieron en familias mono parentales o a cargo de otras personas, todos con historial de consumo de drogas y/o alcohol, algunos se escaparon de su casa antes de los 15 años, tuvieron familiares con historial penitenciario, también mejores amigos que cometieron infracciones contra la Ley Penal y vivían en barrios con pandillas y bandas delictivas 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Identificación de la propensión a la adquisición de un subproducto de una tarjeta de crédito en una entidad bancaria 2\n",
      "3\n",
      "Zarabia Yupanqui, Cinthia 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "El objetivo del presente trabajo es identificar a los clientes con tarjeta de crédito más propensos a la adquisición del subproducto Extra Línea de la tarjeta de crédito en una entidad financiera. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Predicción de adquisición de un préstamo personal bancario a través del canal de televentas utilizando el algoritmo Random Forest 2\n",
      "3\n",
      "De la Cruz Flores, Fiorella Pamela 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "El objetivo de este trabajo es desarrollar un modelo de clasificación binaria para predecir si el cliente va a aceptar o rechazar el préstamo ofrecido por los asesores al contactarlos vía telefónica. Para ello, se utilizó el algoritmo Random Forest que permitió redecir a los clientes con mayor probabilidad a adquirir el producto y así, gestionarlos óptimamente para priorizar su venta. Para el desarrollo del modelo se usó una muestra de seis meses (desde marzo 2017 hasta agosto 2017) de datos de los clientes que cuentan como mínimo con una tarjeta de crédito y tienen un préstamo pre- aprobado con la entidad bancaria. En total la base contó con 991619 registros de clientes 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Diseño muestral de una línea base para caracterizar a los productores de cacao en el Perú 2\n",
      "3\n",
      "Orellano Bedón, María Del Carmen Nataly 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "En muchos países del mundo existen entidades que dentro de sus competencias está el diseño y ejecución de encuestas, censos y otras investigaciones, esto con el fin de obtener información relevante, oportuna y confiable para la toma de decisiones. Inicialmente estos estudios se realizaban siguiendo estrictamente las especificaciones señaladas en los términos de referencia que forman parte del contrato entre la empresa solicitante y la empresa prestadora del servicio. Sin embargo, este procedimiento no garantizaba la obtención de resultados óptimos y por ende ponía en peligro el prestigio de la empresa con los clientes. Ante esta problemática, surgió la necesidad de conducir los estudios siguiendo los pasos de un diseño muestral estándar que tome en cuenta el cumplimiento de los protocolos de seguridad en todas sus etapas. En la memoria del Trabajo de Suficiencia Profesional se expone el caso de una importante institución del estado que contrata los servicios de la empresa para diseñar una línea base para caracterizar a los productores de cacao en el Perú. El objetivo de la línea base es medir los cambios en las variables que la conforman a través de estudios de corte longitudinal. La metodología estandarizada; que en adelante llamaremos diseño muestral estándar, utilizada tiene seis etapas: (1) Identificación de la población de objetivo, (2) Elección del método de recolección de datos, (3) Identificación del marco de muestreo, (4) Selección del método(s) de muestreo y determinación del tamaño de muestra, (5) Planificación del trabajo de campo y aplicación de la prueba piloto, y (6) Ejecución del trabajo de campo 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Identificación de las variables determinantes en el cese voluntario de un colaborador con la regresión de Cox 2\n",
      "3\n",
      "Palomino Gonzales, Javier Norberto 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "En presente trabajo de investigación se explica el desarrollo del modelamiento del evento renuncia voluntaria de los colaboradores de una organización financiera a través del modelo de regresión de Cox. Esta iniciativa fue de mucha utilidad para identificar cuáles son las variablesque determinan este evento y cómo estas impactan en el evento renuncia. Así mismo es muy importante resaltar que para llegar a realizar este estudio, se tuvo que resolver varios desafíos respecto al conocimiento del negocio, así como la obtención los datos a través de una automatización de las consultas a las bases de datos de recursos humanos para obtener datos confiables y relevantes para el estudio. Dicha automatización fue de mucho aporte para la organización, ya que permitió ahorrar 15 días de proceso manual para su obtención, actualmente el proceso manual se ha reducido a actualizar un tablero de indicadores que solo demora 3 minutos en actualizar. Por ello consideramos que el presente estudio representa el trabajo de un año de comprender la dinámica interna de los procesos y a partir de estos proponer nuevas formas de abordar tanto los procesos, así como la formas de analizar los resultados del mismo 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Comportamiento del gasto público de una entidad pública usando el modelo Box - Jenkins, 2009 - 2017 2\n",
      "3\n",
      "Cormán Trujillo, Juan 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Soto Rodríguez, Iván Dennys 5\n",
      "6\n",
      "La entidad pública brinda servicios de capacitación para el Sector Vivienda, Construcción y Saneamiento, inicia su vida institucional en el año 1977. Tiene como finalidad la formación de los trabajadores del sector construcción, la educación superior no universitaria, el desarrollo de investigaciones vinculadas a la problemática de la vivienda y edificación, así como la propuesta de normas técnicas de aplicación nacional. Y se ha establecido en doce sedes a nivel nacional y capacita a más de 30,000 trabajadores en el desarrollo de nuevas tecnologías. La entidad según su estructura organizacional se encuentra conformado por: Consejo Directivo, Gerencia General, Oficina de Administración y Finanzas, Oficina de Planificación y Presupuesto, Gerencia de Formación profesional y Gerencia de Investigación y Normalización; cabe señalar que el Departamento de Informática forma parte de la Oficina de Administración y Finanzas, en la cual se desempeñó el cargo. 6\n",
      "1\n",
      "2020 1\n",
      "2\n",
      "Caracterización del perfil del ingresante de una Universidad Pública aplicando algoritmos clustering K-Prototypes y K- Medoids 2\n",
      "3\n",
      "Chavez Valderrama, Ledvir Ayrton Walter 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "En el presente trabajo de investigación se realizó un estudio comparativo de algoritmos no supervisados para la caracterización del perfil del ingresante de una universidad pública respecto a sus variables sociodemográficas, económicas y de rendimiento académico utilizando algoritmos de segmentación K-prototypes y K-medoids, con el fin de generar conocimientos valiosos y útiles para lograr una mejor comprensión de la diversidad de universitarios que ingresan y con ello conocer el tipo de estudiante que la institución forma, La aplicación se efectuó con datos de alumnos ingresantes a la Universidad Nacional Agraria La Molina de los ciclos académicos 2015-I y 2015-II de las modalidades de Concurso Ordinario y Dos Primeros Puestos de Colegios de Educación Secundaria con un total de 690 postulantes. Se realizó el preprocesamiento de los datos y la aplicación de algoritmos clustering trabajando tanto con variables cuantitativas como cualitativas, para luego determinar el número óptimo de conglomerados y el algoritmo más adecuado utilizando índices de validación interna. Se realizó la validación de los clusters obtenidos de manera univariada (análisis de variancia o ANOVA y prueba Chi cuadrado) y multivariada (algoritmo Boruta y árbol C5.0), por último, se determinó las variables más importantes para caracterizar el perfil de los ingresantes. Con la investigación realizada se logró identificar 3 tipos de alumnos: Ingresante previsto, Ingresante en proceso y el Ingresante en inicio; cada uno con características peculiares, las cuales permitirán a los responsables de las políticas educativas y en especial a los profesores consejeros saber el tipo de alumno que tienen a su cargo desde que ingresa a la universidad y empezar con ello políticas educativas como el emprendimiento del acompañamiento especializado, sistemático e integral; buscando la realización del paradigma del aprendizaje que la universidad se ha propuesto en su Modelo Educativo. 6\n",
      "1\n",
      "2019 1\n",
      "2\n",
      "Cambios en la producción agrícola y el rol de la investigación y extensión agrícola en el Perú: 1950-2011 2\n",
      "3\n",
      "Gómez Galarza, Vilma Elvira 3\n",
      "4\n",
      "Ingeniero Estadístico 4\n",
      "5\n",
      "Sotomayor Ruíz, Rino Nicanor 5\n",
      "6\n",
      "Este trabajo presenta el análisis de 60 años de evolución de la producción del sector agrícola desde 1950 al 2011 y lo relaciona con la investigación y extensión agrícola. Es una investigación no experimental que se expone en tres partes cuyo centro es el análisis del comportamiento de la producción agrícola y los factores: superficie cosechada, rendimientos por ha y la estructura de uso del suelo, luego se relaciona con la investigación y extensión agrícola. Este trabajo ha utilizado más de 200 series estadísticas que fueron sometidos a análisis exploratorio mediante representaciones gráficas de secuencias con diagrama de líneas, análisis de consistencia y homogeneización a través de conversiones y ajustes mediante el promedio trienal móvil y ponderaciones. A las series estadísticas ajustadas se aplicó el Método de los Efectos para cada uno de los 49 cultivos y sus correspondientes grupos. Los resultados de las investigaciones muestran que: (a) El sector agropecuario ha estado supeditado a los modelos de desarrollo que privilegiaron a otros sectores de la economía lo que generó una producción agrícola con más de 30 años de estancamiento continuo en el periodo de 1950-2011. (b) Al relacionar la producción con los factores señalados, los cálculos evidencian que, el factor superficie cosechada ha definido la evolución de la producción agrícola tanto en los periodos de expansión como de estancamiento. Los factores rendimiento y estructura de uso del suelo han jugado un rol secundario y en los periodos de estancamiento ha atenuado la tendencia de descenso de la producción agrícola. (c) Al analizar el factor rendimiento como indicador del aporte de la investigación a la producción agrícola se evidencia su contribución positiva en especial en las épocas de estancamiento a pesar de la débil institucionalidad y del escaso financiamiento del estado y del sector privado. Finalmente se presentan las conclusiones y recomendaciones. 6\n",
      "1\n",
      "2019 1\n",
      "2\n",
      "Variables que explican los rangos remunerativos del primer empleo de los egresados universitarios del Perú aplicando regresión logística ordinal 2\n",
      "3\n",
      "Aquino Gamboa, Juan Carlos 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "En la presente tesis se aplica la regresión logística ordinal, con la finalidad de identificar las variables que mejor explican los rangos de los ingresos de los egresados universitarios del Perú. Para el estudio se usa los datos de la encuesta de egresados universitarios del año 2014 realizada por el INEI. Se ajustan los datos a tres modelos aplicando la regresión logística ordinal con función de enlace logit proporcional acumulativo y para las universidades públicas y privadas. La variable dependiente son los rangos de ingresos (Bajo, Medio, Alto y Muy alto) y los conjuntos de variables independientes agrupadas en cuatro categorías: Grupo A: socio-académicas (7), Grupo B: referidas a la evaluación de las competencias recibidas en la universidad (12), Grupo C: referidas a la importancia de las competencias para su desarrollo profesional (12) y Grupo D: respecto a los profesores de la carrera (5). Para explicar los rangos de los ingresos, la regresión logística ordinal identificó para los tres modelos y para las universidades públicas y privadas, variables significativas socio-académicas: sexo, pertenencia al cuadro de méritos, si obtuvo o no el título profesional, su primer empleo relacionado con la formación profesional. Respecto a la calificación sobre la preparación recibida en la universidad para el desarrollo de las competencias: para coordinar actividades, para los conocimientos básicos de otros campos (públicas) y para el dominio del área de disciplina, el utilizar herramientas informáticas básicas, el utilizar software específico de la carrera (privadas). Respecto a la importancia de las competencias para su experiencia laboral: redactar informes o documentos, tener conocimientos básicos de otros campos o disciplinas (públicas) y rendir bajo presión y cumplir con los objetivos y coordinar actividades (privadas). 6\n",
      "1\n",
      "2019 1\n",
      "2\n",
      "Identificación de las reglas de asociación utilizando los algoritmos secuenciales Spade y GSP 2\n",
      "3\n",
      "Lévano Chiroque, Fernando Thomas 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "Hoy en día los datos secuenciales son de gran importancia, debido a que pueden encontrarse en distintas aplicaciones como: registros de ventas, registros médicos de pacientes, registros webs, bolsa de valores, base de datos en geofísica, etc. Es por esta razón que se han estudiado las tendencias o patrones a través del tiempo con el algoritmo secuencial. En esta investigación se estudia con mayor profundidad el algoritmo secuencial Sequential Pattern Discovery using Equivalent Class (SPADE, por sus siglas en inglés), debido a que usa una eficiente búsqueda de las reglas que se generan por el algoritmo, reduciendo el número de reglas y costos de la memoria. En primer lugar, se ilustra el procedimiento para obtener las reglas de asociación y luego con un conjunto de datos se identifican las reglas de asociación computacionalmente. Por último se compara los resultados obtenidos con el algoritmo Generalized Sequential Patterns (GSP, por sus siglas en inglés), debido a que ambos algoritmos tienen el mismo enfoque. Uno de los resultados más resaltantes fue “el cliente compra pavo en un tiempo máximo de tres meses, dado que compró antes costilla de cordero”. Los resultados que se obtuvieron sirven para incrementar las ventas del establecimiento a través de ventas cruzadas. El algoritmo SPADE permitió obtener reglas más completas que el GSP. 6\n",
      "1\n",
      "2019 1\n",
      "2\n",
      "Estudio de las principales variables que determinan el consumo de una marca de bebida gaseosa usando técnicas discriminantes 2\n",
      "3\n",
      "Alcedo Zambrano, Rosario Jessica 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "La investigación de mercados y el uso de la estadística básica y avanzada en los últimos tiempos han pasado a ser una de las herramientas más importantes para los Jefes y Gerentes de Producto, un claro ejemplo de ello es el presente estudio, que a través del análisis multivariante busca resolver algunas inquietudes. El presente estudio, tiene como finalidad construir un modelo que permita predecir si un consumidor de gaseosas consume la Marca X. El modelo será utilizado para identificar a sus consumidores y conocer sus expectativas, evaluar sus gustos/ preferencias y medir el impacto de la publicidad y las promociones. La data con la que se construyó el modelo, fue proporcionada por la Empresa X, ellos cuentan con información pasada y presente de una serie de variables que son de su interés. Lo que se buscó con este estudio, es aprovechar dicha información para realizar el modelo. Para el análisis se utilizaron dos técnicas discriminantes, Análisis de Regresión Logístico y Arboles de decisión, ambos modelos se construyeron con uso del Enterprise Miner (SAS). El resultado final, fue un árbol de decisión con seis variables independientes, se trata de un modelo fácil de interpretar y aplicar. 6\n",
      "1\n",
      "2018 1\n",
      "2\n",
      "Clasificación de resultados en la prueba de admisión de la UNALM utilizando análisis discriminante lineal de Fisher y Máquina de Soporte Vectorial 2\n",
      "3\n",
      "Vivanco Huaytara, Fredy 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Rosas Villena, Fernando René 5\n",
      "6\n",
      "El objetivo general de la investigación es la identificación entre las técnicas del Análisis Discriminante Lineal de Fisher y Máquina de Soporte Vectorial la que presenta mejores indicadores de clasificación del rendimiento de los postulantes en la prueba de admisión 2015-II en la Universidad Nacional Agraria La Molina (UNALM). Ambas técnicas estadísticas se aplicaron en dos oportunidades, en la primera se evaluó el resultado en la prueba admisión de la UNALM de los postulantes que no se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 1) y en la segunda se evaluó resultado en la prueba admisión de la UNALM de los postulantes que si se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 2). Los resultados muestran que la técnica Máquina de Soporte Vectorial presenta mejores indicadores de clasificación que el Análisis Discriminante Lineal de Fisher. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción metodológica del modelo espacial autorregresivo en el error 2\n",
      "3\n",
      "Polo Lucero, Marco 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "El propósito de este trabajo es presentar y explicar la metodología de un modelo espacial autorregresivo en el error. Para esto se comenzó explicando la teoría necesaria para comprender la estructura que tiene un modelo espacial autorregresivo en el error. En esta parte se estableció que para este modelo es necesario utilizar variables de tipo de corte transversal, que la unidad espacial es un área geográfica delimitada por un polígono y que el comportamiento de la variable de interés debe ser diferente en cada unidad espacial en el área de estudio, esto para no tener problemas de autocorrelación espacial. Finalmente, este modelo utiliza una matriz de pesos que tiene la función de controlar y capturar la autocorrelación espacial, obteniendo un modelo con parámetros estimados insesgados y consistentes.  Para explicar la aplicación del modelo espacial autorregresivo en el error, se utilizaron los resultados de una investigación que se hizo en Argentina acerca de la Fecundidad (promedio de hijos nacidos vivos al nacer por mujer) en mujeres entre 15 y 29 años de edad. Donde primero se obtuvieron estadísticas básicas de las variable dependiente (variable en interés) y de las independientes. Luego se dividió en 531 unidades espaciales (partidos) al territorio Argentino. Después se eligió dos tipos de matrices de pesos (reina y 4-vecinos más cercanos). Posteriormente se probó con la estadística de I Moran, que si existía autocorrelación espacial global utilizando solo la variable fecundidad. Luego se utilizó la variable fecundidad con sus variables explicativas y se concluyó utilizando las pruebas estadísticas LM-ERR y LM-EL que la autocorrelación espacial se encontraba en la estructura del error. Lo anterior sugería que el modelo espacial autorregresivo en el error era el recomendable a utilizar. Finalmente se presentaron y estimaron a los dos modelos espaciales autorregresivos, uno utilizando la matriz de pesos tipo “reina” y el otro modelo utilizando la matriz de pesos tipo “4-vecinos más cercanos”, ambos modelos con variables explicativas significativas y capturando la autocorrelación espacial en la estructura del error,  concluyendo que ambos modelos espaciales autorregresivos en el error son igual de óptimos para el estudio de la Fecundidad en Argentina. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción metodológica del anàlisis Clúster utilizando el algoritmo de Ward 2\n",
      "3\n",
      "Dongo Román, Andie Bryan 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "El presenta trabajo tiene como objetivo principal describir la metodología que se debe seguir al realizar un análisis clúster utilizando el algoritmo de Ward, mostrando una serie de pasos para su correcta aplicación. Además, exponer cuáles son las características y las ventajas de elegir este algoritmo como criterio de agrupamiento.  El algoritmo de Ward es uno de los diversos métodos jerárquicos del análisis clúster, el cual viene a ser uno de lo más usados por tener un fundamento estadístico (mientras que los demás suelen ser heurísticos), pues se basa en el criterio de la suma de cuadrados para medir la proximidad entre clústeres durante el proceso de agrupamiento.  Como ejemplo aplicativo se planteó el caso de las Comunidades Autónomas de España, las cuáles se agruparon en base a la actividad de sus salas de proyección de cine. En este caso, siguiendo los pasos correspondientes, el algoritmo de Ward determinó que estas Comunidades se agrupaban en cuatro clústeres, los cuales mostraron características que los diferenciaban entre sí en función de las variables de estudio. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción del procedimiento metodológico del análisis cluster no jerárquico con el algoritmo Clarans 2\n",
      "3\n",
      "Salvador Alfaro, Carlos Agustín 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "El algoritmo CLARANS, perteneciente a los métodos clúster no jerárquico. Lo que se pretende describir en este trabajo es explicar el procedimiento del algoritmo CLARANS. El proceso que realiza este algoritmo es encontrar una muestra con una cierta aleatoriedad en cada paso de la búsqueda. El agrupamiento obtenido después de sustituirlo a un solo medoide se denomina el vecino del agrupamiento actual. Si en el camino el objeto (individuo) encuentra un mejor vecino, CLARANS lo mueve al nodo del vecino y el proceso comienza de nuevo; si ya no lo encuentra entonces el agrupamiento actual para y se produce un óptimo local (Cluster). Se presenta un ejemplo que ilustra la metodología y se explica el paso a paso del algoritmo CLARANS. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción metodológica del análisis conjunto con perfiles completos 2\n",
      "3\n",
      "Morales Plaza, Sonia Paola 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "El comportamiento del consumidor se ve influenciado por diversos factores al tomar una decisión de compra, generando preferencias por aquel producto o servicio que reúna las características deseadas. Por ello las organizaciones y empresas han enfocado sus esfuerzos por desarrollar productos o servicios basados en las preferencias de sus clientes potenciales. Una técnica estadística multivariante que permite conocer cuáles características de un producto o servicio son las de mayor preferencia por los consumidores, es el Análisis Conjunto.  Este trabajo presenta y describe la metodología de Análisis Conjunto con perfiles completos, desarrollada para conocer la estructura de las preferencias de los consumidores de un modo más cercano a la realidad. Así mismo, se ilustra la utilización de esta técnica, sus variados ámbitos de aplicación, diversos tipos de Análisis Conjuntos existentes y en qué contexto resulta más apropiado aplicarlas. Finalmente, se realiza una aplicación con datos sobre una empresa que desea lanzar al mercado un nuevo producto, con el objetivo de conocer la combinación ideal de características que deba poseer éste para obtener una mayor preferencia por sus clientes potenciales y así desarrollar un diseño de producto eficaz. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Clasificación de fuga de clientes en una entidad financiera utilizando el algoritmo Smote para datos desbalanceados en una regresión logística 2\n",
      "3\n",
      "Pariona Huarhuachi, Jefferson Clauss 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "La retención de clientes ha tomado mucha importancia en los últimos años en las entidades financieras debido a la competencia agresiva por parte del sector, así como la autonomía del cliente en buscar mejores beneficios dentro de todas las ofertas que existen en el mercado bancario lo que se ve reflejado en el aumento de la tasa de clientes fugados. Ante esto se ha visto necesaria la implementación de técnicas estadísticas y/o técnicas de minería de datos, con la finalidad de construir un clasificador predictivo que pueda ayudar a identificar a clientes potenciales a fugarse. En muchos casos cuando se aplican técnicas de clasificación, es común que la clase a predecir ocurra con menor frecuencia que la otra clase: la presencia de datos desbalanceados. Es decir, se tiene menor número de clientes fugados que no fugados, lo cual representa un inconveniente debido a que el clasificador necesita datos suficientes de ambas clases para poder aprender de ellas y así alcanzar una buena predicción. En esta investigación se propone el algoritmo Syntetic Minority Over-sampling Technique (SMOTE) como solución a este problema. SMOTE crea instancias nuevas a partir de un sobre-muestreo de las instancias existentes, llevando la clase minoritaria a un número suficiente para ser considerada balanceada y la clase mayoritaria si es necesaria reducirla mediante sub-muestreo aleatorio. En la presente investigación se validarán tales beneficios con la construcción de un modelo de regresión logística binaria con datos desbalanceados con y sin la aplicación del algoritmo de SMOTE; con el fin predecir la fuga de clientes en una entidad financiera. Se usarán para medir la precisión, la curva ROC y elementos de la comprobación de tabla cruzada como la especificidad y la sensibilidad. 6\n",
      "1\n",
      "2018 1\n",
      "2\n",
      "Segmentación de clientes de un casino utilizando el algoritmo partición alrededor de medoides (PAM) con datos mixtos 2\n",
      "3\n",
      "Elguera Vega, Rhony Miguel 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "En la actualidad, la gran cantidad de datos que se almacenan de los clientes en las diferentes empresas y la capacidad de procesamiento que brindan las computadoras, han generado gran interés por investigar; así como, desarrollar métodos y algoritmos para el análisis de agrupamiento. Los métodos de agrupamiento dirigidos a la segmentación de clientes permiten a las empresas identificar los patrones y perfiles de compra o servicios, ayudando a tomar mejores decisiones de las estrategias de canales y publicidad para sus clientes. En la presente investigación se aplica el método de agrupamiento basado en las particiones de k-Medoides con el algoritmo PAM (Partición Alrededor de Medoides). El algoritmo PAM se basa en particionar el conjunto de datos en k grupos, donde k es conocido; es considerado más robusto ante datos atípicos y el ruido, se basa en minimizar la suma de disimilitudes entre un objeto y el Medoide (centro del grupo). El objetivo de la presente investigación es aplicar el algoritmo PAM para segmentar a los clientes de un casino con los datos obtenidos, a través del uso de tarjetas en el tragamonedas. El método de la silueta permitió identificar tres clústers como el número óptimo. El análisis de agrupamiento con el algoritmo PAM usando la medida de distancia Gower, resultó la segmentación de clientes para los tres clúster con porcentajes de 49.4%, 11.3% y 39.4% respectivamente. La agrupación fue validada, al obtener para las 6 variables cuantitativas todos los ANVAs significativos y con el árbol de clasificación C5.0 un 99.35% de precisión.  Los resultados de la caracterización muestran que el clúster 1 son clientes con valores de los promedios para las 6 variables en un nivel intermedio, el 67.0% son hombres y 100% el tipo de tarjeta es classic. En el clúster 2 están los clientes con los valores más altos en los promedio de las 6 variables, el 59% son hombres y el 100% usan la tarjeta silver. En el clúster 3, se encuentran los clientes con los promedios más bajos, el 64% son hombres y el 100% usan tarjeta classic 6\n",
      "1\n",
      "2018 1\n",
      "2\n",
      "Detección de outliers espaciales utilizando el diagrama de dispersión de Moran y el variograma Nube 2\n",
      "3\n",
      "Palacios Mosquera, Maritza Sarela 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Chue Gallardo, Jorge 5\n",
      "6\n",
      "Uno de los problemas del análisis del datos es la presencia de outliers, esto puede afectar las medidas estadísticas que se desean estimar de una población. La presente investigación se enfoca a la detección de los outliers pero en un contexto geográfico; para ello se empleó los datos obtenidos de la encuesta nacional de los egresados universitarios peruanos en el 2014. Como variable de estudio se consideró el ingreso total de los egresados universitarios en las diferentes regiones del pais para observar si existen ingresos muy atípicos respecto a una región a otra, o si dentro de una región existen valores muy altos respecto a su alrededor, estos valores anormales o raros dentro de un contexto geográfico se consideran como outliers espaciales que es muy diferente a los outliers tradicionales, para poder identificar dichos valores atípicos espaciales se empleó dos técnicas gráficas exploratorias para la detección de outliers espaciales: el variograma nube y el diagrama de dispersión de Morán, que tienen la particularidad de ser muy sensibles a la presencia de outliers espaciales. Se utilizó las dos pruebas los datos del ingreso total de la encuesta, se consideró una muestra 250 datos para su óptimo procesamiento, luego se logró detectar los outliers espaciales de la variable de la investigación que fue de ingreso total de S/7’400, y donde el variograma nube fue más sensible a la presencia de outliers que el diagrama de dispersión de Morán. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción metodológica de las series de tiempo con redes neuronales artificiales 2\n",
      "3\n",
      "Neira Campos, Mike Alex 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "Este trabajo monográfico gira en torno a las series de tiempo con Redes Neuronales Artificiales a fin de realizar pronósticos. Para este propósito, el presente trabajo se compone de 4 capítulos, donde el primer capítulo versa sobre las definiciones y conceptos principales del pronóstico de una serie temporal que otorga validez teórica a la investigación. En el segundo capítulo, el lector podrá encontrar la descripción de las Redes Neuronales Artificiales en la predicción de datos. El tercer capítulo, da cuenta de las implicaciones de una metodología del pronóstico de datos, utilizando las Redes Neuronales Artificiales. Finalmente, el capitulo 4, dilucida la aplicabilidad de las mencionadas Redes y se hace un paralelo con otros métodos de pronóstico con el objeto de resaltar sus diferencias y características. En conclusión, podemos decir que el lector podrá encontrar en este trabajo las etapas necesarias para llevar a cabo la elaboración de una red neuronal que pueda predecir valores futuros de una serie de tiempo. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción metodológica del modelo de ecuaciones estructurales con el método de estimación de mínimos cuadrados parciales 2\n",
      "3\n",
      "Mamani Tone, Edith Rita 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "La presente monografía estudia la Descripción Metodológica del Modelo de Ecuaciones Estructurales con el Método de Estimación de Mínimos Cuadrados Parciales. En el primer capitulo se describe detalladamente la sustentación teórica del Modelo de Ecuaciones Estructurales; en el capítulo 2 se describe el Método de Mínimos Cuadrados Parciales (PLS); en capítulo 3 se describe la Metodología detallando los pasos del Modelo de Ecuaciones Estructurales por el Método de Mínimos Cuadrados Parciales y en el capítulo 4 se observa la Aplicación, paso a paso en un caso para ejemplificar la metodología. Se concluyó los modelos de ecuaciones estructurales SEM es una extensión de la regresión múltiple, se aplica esta técnica para encontrar relaciones entre variables observables y no observables llamadas (latentes) para pasar posteriormente a estimar los parámetros. tiene como objetivo la predicción, no es preciso que los datos provengan de una distribución normal y puede aplicarse a estudios de muestras pequeñas, permite estimar modelos muy complejos con muchas variables latentes y medibles. En la aplicación de la descripción metodológica se realizó un estudio empírico el segundo semestre de 2013 sobre una muestra correspondiente a 300 alumnos universitarios chilenos con acceso a bases de datos científicas. Para los cálculos de PLS se utilizó el software WarpPLS 4.0. Los resultados del ejemplo en el análisis de PLS del caso indicaron la buena capacidad predictiva del modelo de investigación, y a su vez, la explicación del análisis logró ejemplificar en forma clara la metodología propuesta. 6\n",
      "1\n",
      "2018 1\n",
      "2\n",
      "Mixtura finita basada en la distribución Birnbaum-Saunders normal asimétrica 2\n",
      "3\n",
      "Maehara Aliaga, Rocío Paola 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Vargas Paredes, Ana Cecilia 5\n",
      "6\n",
      "Los modelos de mixtura han recibido una gran atención en el área de estadística debido a la amplia gama de aplicaciones encontradas en los últimos años. Por otro lado el modelo Birnbaum-Saunders (BS) surgió en un contexto de fatiga de materiales. Este modelo ha sido aplicado en otras áreas como por ejemplo, ciencias de la salud, ambiental, forestal demográficas, actuarial, financiera, entre otras. Teniendo en cuenta que la distribución Birnbaum-Saunders Normal Asimétrica (BS-NA) es una extensión de la distribución BS, ya que permite predecir percentiles extremos especialmente en la cola izquierza y a su vez modelar datos asimétricos. Este trabajo discute el modelo de Mixtura Finita Birnbaum-Saunders Normal Asimétrica con G componentes, como una extensión del trabajo desarrollado por Benites et al. (2017), Vilca et al. (2011) y Balakrishnan et al. (2011). Esta propuesta es una clase flexible de distribuciones de probabilidad que permite modelar datos con comportamiento asimétrico, que poseen observaciones atípicas y que a su vez son provenientes de poblaciones heterogéneas. Para obtener los estimadores de máxima verosimilitud se usa el algoritmo EM con maximización condicional. Además, la matriz de información empírica se deriva analíticamente para obtener el error estándar. También se realizan estudios de simulación y analizan dos conjuntos de datos reales para ilustrar la utilidad del método propuesto. Finalmente, la propuesta del algoritmo y métodos son implementados en el programa R y posteriormente introducidos en el paquete bssn y en el portafolio GitHub 6\n",
      "1\n",
      "2018 1\n",
      "2\n",
      "Estrategias de mercado en un centro educativo privado en la localidad de Arequipa 2\n",
      "3\n",
      "Linares Torres, Miguel Angel 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Espinoza Villanueva, Luis Enrique 5\n",
      "6\n",
      "Se realizó una investigación cuantitativa, con el fin de identificar los factores más influyentes al momento de evaluar el servicio que ofrece el centro educativo, el instrumento de medición es la encuesta vía telefónica y se  entrevistó a los  padres de familia que matricularon a sus hijos en el año 2016. La recolección de información se realizó por medio de un cuestionario,  seguidamente se analizó en una base de datos y con procedimientos estadísticos se ejecutó pruebas para determinar  qué factores  son los más influyentes al momento de evaluar la calidad del servicio. Asimismo, se analizó el entorno como la infraestructura, calidad de servicio, equipamiento tecnológico y calidad educativa.  Finalmente con los resultados de las pruebas se procedió a elaborar estrategias de mercado para un óptimo posicionamiento 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Descripción de la metodología de análisis de cluster con algoritmo Fuzzy C-means 2\n",
      "3\n",
      "Flores Bellido, Giovanna 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "6\n",
      "En el presente trabajo se presenta la metodología del algoritmo Fuzzy C-means para el análisis de cluster, el cual fue presentado por Bezdek y Dunn en 1973, la cual combina los métodos basados en la función objetivo con los de la lógica Fuzzy término presentado por Lofty Zadeh en 1960 como medio para modelar la incertidumbre a través de las etapas de fuzzificación, reglas de evaluación y defuzzificación. El algoritmo Fuzzy C-means realiza la formación de cluster a través de una partición suave de los datos, es decir para realizar del reconocmiento de patrones a través del hallazgo de los grados de pertenencia de cada individuo a los diferentes cluster, donde un individuo no tendría pertenecía exclusiva a un solo grupo, sino que un individuo podría tener grados de pertenencia a distintos grupos, a diferencia de otros métodos que realizan la formación de los cluster basados en la lógica binaria o partición dura. Utilizando el software estadístico R se realizó la aplicación del algoritmo Fuzzy C-means sobre datos de jugadores para la formación de cluster a través de rapidez y resistencia 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Análisis de datos en una auditoría de mercado para productos de consumo masivo en bodegas de Lima Metropolitana 2\n",
      "3\n",
      "Recuay Denegri, Briggitt Giuliana 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Gonzáles Chavesta, Celso 5\n",
      "6\n",
      "Ante un mercado cambiante las empresas del rubro de consumo masivo se encuentran sumergidas en una serie de interrogantes en cuanto a la participación de sus productos. Para ello aplican herramientas como la auditoría de mercado que a través del resultado de una de sus etapas el “análisis de datos” encuentran confianza a la hora de tomar decisiones. Este estudio presenta el proceso del análisis de datos en una auditoría de mercado para productos de consumo masivo en el canal tradicional. Los productos analizados fueron las cremas dentales, los desodorantes y los energizantes. Se trabajó con una muestra de 1200 bodegas ubicadas dentro de Lima Metropolitana. El conjunto de datos contiene variables cuantitativas como las ventas, las compras, los precios y los inventarios, además de las variables cualitativas como son los atributos (marca, tamaño, sabor, etc.) de las variedades. Los datos fueron recolectados de las bodegas el primer semestre del año 2016 vía celular mediante el aplicativo de relevamiento “Audit” para luego ser cargados al sistema de trabajo. Culminada cada fase del análisis de datos (limpieza de datos, análisis de las variables y visualización de resultados) se supervisaron las bodegas cuyas variedades presentaron incoherencias en sus datos y se procedió a corregir. Para los tres productos en su mayoría se encontraron: ausencia de datos en la variable compra e inventario y errores en los precios y ventas. Estos errores suelen suceder debido a una mala digitación (error de codificación) o a la omisión de parte del supervisor al momento de ingresar los datos al aplicativo. Como resultado del estudio se obtuvieron datos consistentes para las cremas dentales, desodorantes y energizantes, los cuales fueron de mucha relevancia para las empresas. Otros productos de consumo masivo pueden seguir también la misma estructura para el análisis de sus datos. 6\n",
      "1\n",
      "2017 1\n",
      "2\n",
      "Análisis y diseño de un sistema distribuido de pago middleware orientado a la mensajería entre una entidad bancaria y una empresa farmacéutica 2\n",
      "3\n",
      "Reyna Colona, Tino Fabricio 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Menacho Chiok, César Higinio 5\n",
      "6\n",
      "En este tiempo presente, las empresas desarrollan diversos planes de trabajo conjunto de negocio, a modo de corporaciones, joint ventures, etc., y surge la necesidad de manejo eficiente y ordenado de volúmenes grandes de datos e intercambio de información entre las corporaciones con los fines de negocio visibles en el ámbito comercial. En tal sentido, el trabajo de investigación provee una metodología de desarrollo de un sistema de integración basado en Middleware Orientado a la Mensajería (MOM) que media y favorece la integración financiera entre dos entidades de ejemplo, una empresa farmacéutica y un banco. El propósito del trabajo de investigación es que, con la metodología presentada, se identifiquen los procesos clave y la dinamización de los mismos para representar el flujo de transacciones electrónicas para la debida integración entre las dos entidades señaladas anteriormente. La metodología consta de las secciones de: análisis del sistema, diseño del sistema, arquitectura y secciones referidas a la interconexión, mensajería e interfaces gráficas de usuario para la debida gestión del sistema. Los resultados de la aplicación de esta metodología son: el reconocimiento de los procesos clave según los objetivos del trabajo, la presentación de la dinamización de tales procesos, y comprender de manera preliminar lo referido a la interconexión entre las entidades presentadas, empresa farmacéutica y banco, definición de mensajes para las transacciones financieras entre las entidades señaladas y la visualización de la gestión del sistema por medio de interfaces gráficas de usuario. Tales resultados conducen a la idoneidad de la metodología en todas las secciones presentadas, abriendo el espacio para posteriores oportunidades de investigación en banca electrónica. 6\n",
      "1\n",
      "2013 1\n",
      "2\n",
      "Análisis del coste de los siniestros en una compañía de seguros utilizando las distribuciones asimétricas Skew-Normal y Skew-T 2\n",
      "3\n",
      "Mendoza  Quevedo, Diego Alonso 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "López de Castilla Vásquez, Carlos 5\n",
      "6\n",
      "La presente investigación tiene por objetivo principal determinar si las distribuciones asimétricas skew-normal y skew-tson buenos modelos para describir datos relativos al coste de los siniestros. Para lo cual se analizaron dos conjuntos de datos de una compañía de seguros, ajustando las distribuciones en estudio y las tradicionales a estos datos. Luego se compararon las distribuciones empleando el Criterio de Información de Akaike (AIC) y del Logaritmo de la función de Verosimilitud, complementados con la prueba de bondad de ajuste Kolmogorov-Smirnov,obteniendo resultados positivos. Además, se calcularon medidas de riesgo como el Valor en Riesgo (VaR) y el Valor en Riesgo Condicional (TVaR), que brindaron mayor solidez a los resultados previos: los costes de los siniestros en los seguros se ajustan bien a las distribuciones asimétricas skew-normal y skew-t. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Estimación de la incertidumbre asociado al método de ensayo para el análisis de ácido carmínico en cochinilla 2\n",
      "3\n",
      "Morán Huamani, Violeta 3\n",
      "4\n",
      "Ingeniero Estadístico Informático 4\n",
      "5\n",
      "Sotomayor Ruíz, Rino Nicanor 5\n",
      "6\n",
      "De acuerdo a la Norma NTP /IEC/ISO /17025 de Requisitos Generales relativos a la competencia de los laboratorios de ensayos y calibración pone especial énfasis en la necesidad de estimar la incertidumbre asociada con calibraciones internas. La presente monografía se describe un procedimiento para la estimación de la Incertidumbre en el método de ensayo en el análisis del Ácido Carmínico en Cochinilla. El cálculo de la Incertidumbre está basado en el Método de Evaluación (de incertidumbre) Tipo A y Método de Evaluación (de incertidumbre) Tipo B. Estos métodos consisten en la identificación y cuantificación de las diferentes fuentes de incertidumbre tanto internas como externas. Hallando la incertidumbre estándar de la calibración, incertidumbre estándar de la medida de la masa, incertidumbre estándar por deriva de la balanza. Se ilustran lo factores que más influyen en este tipo de análisis como son los equipos e instrumentos de medición utilizados; y la destreza de los ejecutores del ensayo, bajo los conceptos estadísticos como son la responsabilidad del método. De igual forma se presenta la descripción de la metodología utilizada donde se describe los parámetros metrológicos usados. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Segmentación de usuarios de productos tecnológicos a partir de valores y actitudes 2\n",
      "3\n",
      "Flores Espinoza, María del Carmen 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El presente trabajo tiene como propósito esencial, realizar una segmentación psicográfica, en base a valores y estilos de vida, así como las actitudes frente a la tecnología de los habitantes de Lima Metropolitana y Callao de los niveles socioeconómicos A, B, C y D, entre 15 y 69 años de edad. Se utilizó una muestra de 906 casos, a los cuales se le aplicó un cuestionario estructurado que contenía 35 atributos previamente definidos en base a una investigación cualitativa, y otras preguntas de control, tal como edad, nivel socioeconómico y tenencia de productos tecnológicos en el hogar. Del total de 35 atributos evaluados los cuales se redujeron en 8 factores tras realizar un análisis factorial. Seguidamente se realizó un análisis clúster, donde se identificaron 5 segmentos caracterizados por los atributos previamente mencionados: Aquellos que se encuentran Orientados a la tecnología; los que, a pesar de tener productos tecnológicos prefieren estar esconectados; los que se encuentran Orientados al poder, los que tienden a ser Tradicionales por sus actitudes frente a las costumbres, y aquellos Orientados a los valores que tienen como eje virtudes tales como la solidaridad, sencillez, etc. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Clasificación de familias en Cajamarca según su situación económica mediante el análisis de conglomerados 2\n",
      "3\n",
      "Vicente Vasquez, Juana Mercedes 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El estudio tiene como objetivo clasificar a las familias encuestadas en los distritos de San Pablo, San Luis y San Bernardino de la provincia de San Pablo en el departamento de Cajamarca según un conjunto de variables socio-económicas. Estos datos corresponden a una investigación realizada por un grupo de personas que laboran en la Universidad del Pacifico, la encuesta fue realizada en Diciembre del 2006. Se desea clasificar a las familias para poder brindar un mejor control en el estudio longitudinal de los proyectos a ser evaluados. Para esto, al culminar la encuesta se planteó una clasificación preliminarmente la existencia de 4 grupos de familias. Para verificar esta clasificación se utilizó el “Análisis de Clúster”, que es un método multivariado de clasificación. Para el procesamiento de los datos se utilizó el programa “Minitab versión 17” y “Microsoft Excel” 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Determinación de perfiles de turistas nacionales de los niveles socioeconómicos medio y alto mediante el análisis conglomerado bietápico 2\n",
      "3\n",
      "Porras Huamán, Beatriz Eta 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El turismo en el Perú ha venido creciendo rápidamente en los últimos años MINCETUR - Plan Estratégico Nacional de Turismo 2012 - 2021 – PENTUR, pág. 11). En el proceso de la investigación turística se recopila los datos, para ser analizados y efectuar una crítica, con fines de alcanzar elementos de trabajo hacia otras fases de desarrollo turístico, las mismas que permitirán obtener nuevos conocimientos que como aporte se utilizarán en la actividad turismo.En este sentido el presente trabajo busca caracterizar o establecer cuáles son los perfiles de los turistas internos en el Perú de los niveles socioeconómicos medio y alto con la finalidad de diversificar la estrategia turística y de este modo poder administrar en forma sostenida recursos del sector público como el privado. Esta investigación utiliza una muestra de 2,400 turistas de los principales destinos turísticos del Perú. El objetivo planteado para el desarrollo del presente trabajo es la determinación de los grupos de individuos observando sus características socio-demográficas, los hábitos de vida y las preferencias con respecto a viajes al interior del país. La metodología empleada corresponde a la técnica de agrupamiento Análisis de Conglomerados Bietápico. La pregunta de investigación planteada para el desarrollo del proyecto se orienta a determinar los diversos tipos de perfiles de turistas nacionales en los niveles socioeconómicos A, B y C. Finalmente se determinó la homogeneidad de los grupos en función a la variabilidad intra-grupos, encontrándose dos grupos, descritos estos en función a los estadísticos encontrados dentro de cada grupo. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Modelación de la volatilidad del índice general de la Bolsa de Valores de Lima, periodo 2009-2011 2\n",
      "3\n",
      "Castillo Gamarra, Jorge Enrique 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Sotomayor Ruíz, Rino Nicanor 5\n",
      "6\n",
      "El presente trabajo tiene como objetivo describir los modelos de varianza condicional ARCH y GARCH junto con sus propiedades y demostraciones, estos modelos se aplican en series de tiempo financieras, debido a que estas presentan como característica principal una fuerte volatilidad con periodos de calma o agitación, lo cual no permite utilizar los modelos de series de tiempo tradicionales que asumen varianzas constantes. Así mismo se realizó una aplicación utilizando como variable el valor diario del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2009 – 2011, para la aplicación  se utilizó el software econométrico Eviews 7. Al analizar los resultados de las estimaciones de los modelos que explicarían la volatilidad diaria de la Rentabilidad del Índice General de la Bolsa de Valores (RIGBVL), periodo 2009 – 2011, se  concluyó que  el  modelo GARCH (1,1) es el adecuado, debido a que el modelo GARCH (1,1) tiene a diferencia de los demás modelos el menor valor tanto en el criterio de información de Akaike (AIC) como en el criterio de información de Schwarz. Previamente se modeló la media de la RIGVBL con el modelo AR (1) 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Identificación de perfiles de clientes crediticios aplicando técnicas de segmentación y regresión logística multinomial 2\n",
      "3\n",
      "Ramírez Soplin, Magally Loidit 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El presente estudio de investigación se centró en identificar los perfiles más adecuados, en una muestra de 8, 504 clientes que realizaron transacciones crediticias en el primer trimestre del año. Se agruparon los casos mediante las técnicas de segmentación: K-means, Bietápico y Kohonen, utilizando variables cuantitativas y categóricas. De las tres técnicas, la que obtuvo mayor medida de silueta de cohesión y separación, fue K-means, indicando una estructura “buena” en cuanto a la cohesión al interior de los grupos y la separación de los mismos. Por otro lado, también se analizó las proporciones de los conglomerados, siendo la técnica K-means la que presentó las proporciones más adecuadas en función a las variables de historial crediticio y transacciones realizadas. Posterior a la obtención de los conglomerados, se procedió al proceso de obtención de la reglas de clasificación, mediante la técnica de regresión logística multinomial, la cual nos permitirá realizar predicciones futuras. El procedimiento se aplicó a la muestra particionada, es decir, una parte de entrenamiento y otra de comprobación. Finalmente, se obtuvo una adecuada tasa de eficiencia en ambas muestras. Además, los análisis permitieron identificar a dos conglomerados que muestran una alerta para la empresa, es decir necesitan ser gestionados de forma oportuna, ya que constituyen un futuro comportamiento de no pago de acuerdo a la caracterización obtenida de dichos conglomerados. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Predicción del rendimiento en el exámen de admisión a la UNALM [Universidad Nacional Agraria La Molina] utilizando las técnicas de análisis discriminante lineal y análisis discriminante con algoritmos genéticos 2\n",
      "3\n",
      "Rado Huaringa, Joao Manuel 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Salinas Flores, Jesús Walter 5\n",
      "6\n",
      "El objetivo de la investigación fue probar la hipótesis que la tasa de error de clasificación utilizando  el análisis discriminante con algoritmos genéticos es menor a la que se obtiene con el análisis discriminante lineal de Fisher. La aplicación se efectuó en la predicción del rendimiento en el examen de admisión de la Universidad Nacional Agraria La Molina de los postulantes cuya preparación se realizó en su Centro de Estudios Preuniversitarios. En la técnica de algoritmos genéticos  se empleó el método de selección, cruce y mutación que permitió realizar la búsqueda de funciones discriminantes con error mínimo. Los resultados del estudio indican que el análisis discriminante con algoritmos genéticos proporcionó una función discriminante más eficiente que la proporcionada por Fisher. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Identificación de un modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria aplicando regresión logística y árboles de clasificación CART 2\n",
      "3\n",
      "Huamaní Miranda, María Alejandra 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "La realidad competitiva que en estos días enfrentan las entidades bancarias ha provocado que éstas no sólo concentren sus esfuerzos de marketing exclusivamente en estrategias de captación de clientes, sino también en estrategias de retención y ﬁdelización; la fuga de clientes es una situación que afecta la rentabilidad de la gran mayoría de las instituciones bancarias dado que se invierte mucho más en la captación de clientes que en campañas para la retención, por ello, es un tema de intensivo estudio cientíﬁco en los últimos años. Las entidades bancarias requieren contar con herramientas que les permitan estimar probabilidades de fuga para su cartera de clientes y así decidir sobre que clientes concentrar sus esfuerzos de retención. En el presente trabajo se utilizó la regresión logística de respuesta binaria  y el algoritmo de árbol de clasificación CART para predecir y clasificar a los clientes con riesgo de fuga y así identificar el mejor modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria. El modelo que mejor explica el riesgo de fuga de un cliente fue la Regresión Logística binaria que obtuvo como variables predictoras número de transacciones, ingreso bruto, número de tarjetas usadas y línea de crédito. Las variables identificadas permitirán a la entidad bancaria reorientar las estrategias en las campañas de retención de clientes. 6\n",
      "1\n",
      "2014 1\n",
      "2\n",
      "Perfil de los clientes que aceptan una tarjeta de crédito de un banco via Call Center utilizando el algoritmo Chaid exhaustivo 2\n",
      "3\n",
      "Acosta Pizarro, Diana Rosa 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El presente estudio tiene como objetivo principal identificar el perfil de los clientes del departamento de Lima que aceptan una tarjeta de crédito de una entidad financiera cuando el productoes ofrecido por el canal de ventas Call Center. Se utilizó la técnica de Árboles de Clasificación CHAID Exhaustivo el cual proporciona buenos resultados de clasificación correcta de los clientes que aceptan una tarjeta de crédito vía Call Center. Se consideró una muestra en un período de cinco meses (Diciembre 2013 a Abril 2014) logrando identificar que las variables más significativas que aportan en el modelo son la edad, el ingreso neto mensual y el tipo de tarjeta que se le ofrece al cliente. Estas variables presentan importancia relevante en el cliente para tomar la decisión de aceptar una tarjeta de crédito. Los resultados obtenidos mediante el algoritmo CHAID Exhaustivopermitieron identificar los patrones que definen el perfil de los clientes que aceptan una tarjeta de crédito vía Call center con el fin de ser más efectivos, aumentando el número de ventas, reduciendo el número de llamadas, minimizando costos y tiempo 6\n",
      "1\n",
      "2015 1\n",
      "2\n",
      "Uso del criterio AHP para la toma de decisiones 2\n",
      "3\n",
      "Loaiza Alamo, Marco Antonio 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "Sotomayor Ruíz, Rino Nicanor 5\n",
      "6\n",
      "El objetivo de esta investigación es determinar la mejor selección de los siete programas especializados para la implementación de un laboratorio mediante la aplicación del Análisis de Proceso Jerárquico (AHP). Con esta técnica se logró un consenso para identificar cuáles son los criterios y las alternativas más relevantes para la toma de decisiones. Para validar el AHP se necesitó los Índices de Consistencia: el Cociente de Resistencia (CR) para la matriz de comparación de criterios y alternativas por pares, el Índice de Consistencia Geométrica (GCI) y el Indicador de Consenso AHP (S*) para la matriz consolidada. Con el desarrollo del algoritmo de AHP se optó por cinco criterios y cuatro alternativas para la toma de decisión. Se concluyó que la alternativa A, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y Microsoft Project, fue la más importante. No obstante, no hubo diferencia significativa considerable con la alternativa C, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y QlikView. En base a los resultados obtenidos se concluye que los programas más adecuados para la implementación del laboratorio informático son: Minitab, SPSS, SQL, Eviews, Microsoft Proyect y QlikView 6\n",
      "1\n",
      "1980 1\n",
      "2\n",
      "Muestreo de unidades agrícolas a través de puntos fijados aleatoriamente 2\n",
      "3\n",
      "Alarcón Novoa, Jorge Alfonso 3\n",
      "4\n",
      "Ingeniero Estadístico 4\n",
      "5\n",
      "Rubio Donet, Arturo 5\n",
      "6\n",
      "En esta investigación se presenta un método de selección de unidades de análisis a través de puntos fijados aleatoriamente en un mapa como marco de muestreo, el cual puede ser utilizado en estudio de muestreo que presentan limitaciones drásticas sobre la inexistencia de padrones o listas de las unidades de muestreo. Asimismo, los parámetros son estimados a través de funciones que respetan las probabilidades asignadas a las unidades seleccionadas, consignando sus indicadores de confiabilidad respectivos. Finalmente, son estimados tamaños de muestra alternativos para futuros trabajos que pudieran implementarse en el área de aplicación. La presente metodología propuesta tiene su aplicación experimental en el Valle de Cañete para lo cual se estima: - Superficie total sembrada con papa en la Campaña 1978 - Cantidad total de semilla de papa utilizada en la Campaña 1978 - Producción total de papa obtenida en la misma Campaña 6\n",
      "1\n",
      "2013 1\n",
      "2\n",
      "Modelo predictivo de quiebre de stock en un supermercado comparando dos métodos de selección de variables 2\n",
      "3\n",
      "Montaño Miranda, Beatriz del Carmen Lidia 3\n",
      "4\n",
      "Ingeniero Estadístico e Informático 4\n",
      "5\n",
      "6\n",
      "El presente estudio tuvo como objetivo principal verificar la disponibilidad de productos en el almacén de un Supermercado, así las decisiones a tomar ante la falta de productos serían más certeras y se tendría un mejor panorama al respecto de la situación del abastecimiento del Supermercado. El trabajo consiste en un modelo de predicción de quiebres de stock para un supermercado. Se analizaron las ventas en unidades, el stock de los productos, los despachos de proveedores, los días del mes de Agosto (con ventas y sin ellas) de un grupo de productos de diferentes gerencias del supermercado (Abarrotes Comestibles, Abarrotes no Comestibles y Bebidas). Con la información recopilada se consideró el valor de la variable dependiente (en quiebre con valor 1 y O en caso contrario). La selección de variables por Boruta permitió obtener un modelo con menos cantidad de variables y con un mejor ajuste al realizar el análisis usando la regresión logística en comparación con la selección de variables por Stepwise. 6\n"
     ]
    }
   ],
   "source": [
    "# Buscar valores de enlace dentro de la entrada de URL\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "# Ignorar los errores del certificado SSL\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "i=url\n",
    "b=0\n",
    "i=i+\"/recent-submissions?offset=\"\n",
    "uni=[]\n",
    "titu=[]\n",
    "tesis=[]\n",
    "grad=[]\n",
    "ases=[]\n",
    "resu=[]\n",
    "año=[]\n",
    "b=0\n",
    "mas=\"?show=full\"\n",
    "for z in range(paginas):\n",
    "    b=str(b)\n",
    "    pag=i+b\n",
    "    html = urllib.request.urlopen(pag, context=ctx).read()\n",
    "    links = re.findall(b'href=.*/(handle/.*/\\d*)\".*Z3988', html)\n",
    "    for link in links:\n",
    "        o=linkss+link.decode()+mas\n",
    "        a=1\n",
    "        for z in range(6):\n",
    "            if a==1:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*dc.date.issued.*break.>(.*)</td>.*<td>', html)\n",
    "            if a==2:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*dc.title.*break.>(.*)</td><td>', html)\n",
    "            if a==3:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*dc.contributor.author.*break.>(.*)</td><td>', html)\n",
    "            if a==4:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*thesis.degree.name.*break.>(.*)</td><td>', html)\n",
    "            if a==5:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*dc.contributor.advisor.*break.>(.*)</td><td>', html)\n",
    "            if a==6:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'td.class.*dc.description.abstract.*break.>(.*)</td>.*<td>.*PE', html)\n",
    "            if a==7:\n",
    "                html = urllib.request.urlopen(o, context=ctx).read()\n",
    "                html = BeautifulSoup(html, 'html.parser')\n",
    "                html=str(html)\n",
    "                links = re.findall(r'tr.class.*odd.*\\s+.*dc.source.*>(.*)</td><', html)\n",
    "            ñ=len(links)\n",
    "            if ñ==0:\n",
    "                ases.append(0)\n",
    "            else:\n",
    "                for link in links:\n",
    "                    f=link\n",
    "                    print(f)\n",
    "                    if a==1:\n",
    "                        año.append(f)\n",
    "                    elif a==2:\n",
    "                        titu.append(f)\n",
    "                    elif a==3:\n",
    "                        tesis.append(f)\n",
    "                    elif a==4:\n",
    "                        grad.append(f)\n",
    "                    elif a==5 and ñ==1:\n",
    "                        ases.append(f)\n",
    "                    elif a==5 and ñ==2:\n",
    "                        c=links[0]+\" y \"+links[1]\n",
    "                        ases.append(c)\n",
    "                        links.pop(1)\n",
    "                    elif a==6:\n",
    "                        resu.append(f)\n",
    "                    elif a==7:\n",
    "                        uni.append(f)\n",
    "            a=a+1\n",
    "    b=int(b)\n",
    "    b=b+20\n",
    "datos = {\n",
    "   # 'Universidad' : uni,\n",
    "    'Titulo': titu,\n",
    "    'Tesista': tesis,\n",
    "    'Grado' : grad,\n",
    "    'Asesor' : ases,\n",
    "    'Resumen' : resu,\n",
    "    'Año' : año\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(datos)\n",
    "df.to_csv(\"TRABAJOFINAlp2.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a72a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Tesista</th>\n",
       "      <th>Grado</th>\n",
       "      <th>Asesor</th>\n",
       "      <th>Resumen</th>\n",
       "      <th>Año</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identificación de clientes en campañas para un...</td>\n",
       "      <td>Alvarez Chancasanampa, Julio César</td>\n",
       "      <td>Ingeniero Estadístico Informático</td>\n",
       "      <td>Miranda Villagómez, Clodomiro Fernando</td>\n",
       "      <td>La presente investigación, tiene como objetivo...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Implementación de una solución de business int...</td>\n",
       "      <td>Tinco Curi, Elizabeth Irene</td>\n",
       "      <td>Ingeniero Estadístico Informático</td>\n",
       "      <td>Soto Rodríguez, Iván Dennys</td>\n",
       "      <td>Actualmente el sector salud dispone de una gra...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Identificación de clientes que realizaron fuga...</td>\n",
       "      <td>Marquez Meza, Francisco</td>\n",
       "      <td>Ingeniero Estadístico Informático</td>\n",
       "      <td>Chue Gallardo, Jorge</td>\n",
       "      <td>El presente trabajo tiene como objetivo ilustr...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estimación del monto de siniestros ocurridos y...</td>\n",
       "      <td>Alarcón Pimentel, Sandra Elena</td>\n",
       "      <td>Ingeniero Estadístico e Informático</td>\n",
       "      <td>Menacho Chiok, César Higinio</td>\n",
       "      <td>El sector asegurador peruano es supervisado en...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Registro de los planes operativos y estratégic...</td>\n",
       "      <td>Flores Santos, José Alberto</td>\n",
       "      <td>Ingeniero Estadístico Informático</td>\n",
       "      <td>Soto Rodríguez, Iván Dennys</td>\n",
       "      <td>El Centro Nacional de Planeamiento Estratégico...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Identificación de un modelo explicativo de ret...</td>\n",
       "      <td>Huamaní Miranda, María Alejandra</td>\n",
       "      <td>Ingeniero Estadístico e Informático</td>\n",
       "      <td>NaN</td>\n",
       "      <td>La realidad competitiva que en estos días enfr...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Perfil de los clientes que aceptan una tarjeta...</td>\n",
       "      <td>Acosta Pizarro, Diana Rosa</td>\n",
       "      <td>Ingeniero Estadístico e Informático</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El presente estudio tiene como objetivo princi...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Uso del criterio AHP para la toma de decisiones</td>\n",
       "      <td>Loaiza Alamo, Marco Antonio</td>\n",
       "      <td>Ingeniero Estadístico e Informático</td>\n",
       "      <td>Sotomayor Ruíz, Rino Nicanor</td>\n",
       "      <td>El objetivo de esta investigación es determina...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Muestreo de unidades agrícolas a través de pun...</td>\n",
       "      <td>Alarcón Novoa, Jorge Alfonso</td>\n",
       "      <td>Ingeniero Estadístico</td>\n",
       "      <td>Rubio Donet, Arturo</td>\n",
       "      <td>En esta investigación se presenta un método de...</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Modelo predictivo de quiebre de stock en un su...</td>\n",
       "      <td>Montaño Miranda, Beatriz del Carmen Lidia</td>\n",
       "      <td>Ingeniero Estadístico e Informático</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El presente estudio tuvo como objetivo princip...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titulo  \\\n",
       "0   Identificación de clientes en campañas para un...   \n",
       "1   Implementación de una solución de business int...   \n",
       "2   Identificación de clientes que realizaron fuga...   \n",
       "3   Estimación del monto de siniestros ocurridos y...   \n",
       "4   Registro de los planes operativos y estratégic...   \n",
       "..                                                ...   \n",
       "57  Identificación de un modelo explicativo de ret...   \n",
       "58  Perfil de los clientes que aceptan una tarjeta...   \n",
       "59    Uso del criterio AHP para la toma de decisiones   \n",
       "60  Muestreo de unidades agrícolas a través de pun...   \n",
       "61  Modelo predictivo de quiebre de stock en un su...   \n",
       "\n",
       "                                      Tesista  \\\n",
       "0          Alvarez Chancasanampa, Julio César   \n",
       "1                 Tinco Curi, Elizabeth Irene   \n",
       "2                     Marquez Meza, Francisco   \n",
       "3              Alarcón Pimentel, Sandra Elena   \n",
       "4                 Flores Santos, José Alberto   \n",
       "..                                        ...   \n",
       "57           Huamaní Miranda, María Alejandra   \n",
       "58                 Acosta Pizarro, Diana Rosa   \n",
       "59                Loaiza Alamo, Marco Antonio   \n",
       "60               Alarcón Novoa, Jorge Alfonso   \n",
       "61  Montaño Miranda, Beatriz del Carmen Lidia   \n",
       "\n",
       "                                  Grado  \\\n",
       "0     Ingeniero Estadístico Informático   \n",
       "1     Ingeniero Estadístico Informático   \n",
       "2     Ingeniero Estadístico Informático   \n",
       "3   Ingeniero Estadístico e Informático   \n",
       "4     Ingeniero Estadístico Informático   \n",
       "..                                  ...   \n",
       "57  Ingeniero Estadístico e Informático   \n",
       "58  Ingeniero Estadístico e Informático   \n",
       "59  Ingeniero Estadístico e Informático   \n",
       "60                Ingeniero Estadístico   \n",
       "61  Ingeniero Estadístico e Informático   \n",
       "\n",
       "                                    Asesor  \\\n",
       "0   Miranda Villagómez, Clodomiro Fernando   \n",
       "1              Soto Rodríguez, Iván Dennys   \n",
       "2                     Chue Gallardo, Jorge   \n",
       "3             Menacho Chiok, César Higinio   \n",
       "4              Soto Rodríguez, Iván Dennys   \n",
       "..                                     ...   \n",
       "57                                     NaN   \n",
       "58                                     NaN   \n",
       "59            Sotomayor Ruíz, Rino Nicanor   \n",
       "60                     Rubio Donet, Arturo   \n",
       "61                                     NaN   \n",
       "\n",
       "                                              Resumen   Año  \n",
       "0   La presente investigación, tiene como objetivo...  2022  \n",
       "1   Actualmente el sector salud dispone de una gra...  2022  \n",
       "2   El presente trabajo tiene como objetivo ilustr...  2020  \n",
       "3   El sector asegurador peruano es supervisado en...  2021  \n",
       "4   El Centro Nacional de Planeamiento Estratégico...  2021  \n",
       "..                                                ...   ...  \n",
       "57  La realidad competitiva que en estos días enfr...  2014  \n",
       "58  El presente estudio tiene como objetivo princi...  2014  \n",
       "59  El objetivo de esta investigación es determina...  2015  \n",
       "60  En esta investigación se presenta un método de...  1980  \n",
       "61  El presente estudio tuvo como objetivo princip...  2013  \n",
       "\n",
       "[62 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"TRABAJOFINAlp2.csv\",sep=\";\",index=False,encoding=\"utf-8\")\n",
    "a=pd.read_csv(\"TRABAJOFINAlp2.csv\",sep=\";\",encoding=\"utf-8\",na_values=[\"0\"])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
